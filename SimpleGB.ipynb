{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "## Реализация градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках этой задачи нужно написать градиентный бустинг над решающими деревьями в задаче классификации. В качестве функции потерь предлагается взять **log loss**. Про него можно прочитать подробнее здесь: https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$y_i$ это правильный ответ (0 или 1), $\\hat{y}_i$ это ваше предскзаание\n",
    "\n",
    "Может показаться, что надо максимизировать функцию $L(\\hat{y}, y) = \\sum_{i=1}^n y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$, где $y_i$\n",
    "\n",
    "Да, но нет. Лучше максимизировать функцию $L(\\hat{y}, y) = \\sum_{i=1}^n y_i \\log(f(\\hat{y}_i)) + (1 - y_i) \\log(1 - f(\\hat{y}_i))$, где $f(x) = \\frac{1}{1 + e^{-x}}$. Благодаря этому у вас не будет ограничений на принимаеммые значения для $\\hat{y}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Напишите вычисление производной f(x), обычно её называют **сигмоида**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return np.exp(-x)/(1+np.exp(-x))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(0) == 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(np.array([0, 0])) == np.array([0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(np.log(3)) == 0.1875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9212\n"
     ]
    }
   ],
   "source": [
    "print(round(der_sigmoid(np.array([-10, 4.1, -1, 2])).sum() + sigmoid(0.42), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, теперь мы умеем считать производную функции f, но надо найти производную log loss-а по $\\hat{y}$ в первом варианте \n",
    "\n",
    "Напоминание, первый вариант это  $y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Напишите вычисление производной log loss-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_log_loss(y_hat, y_true):\n",
    "    \"\"\"\n",
    "    0 < y_hat < 1\n",
    "    \"\"\"\n",
    "    return y_true/y_hat-(1-y_true)/(1-y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2350037122015945, 0.6224593312018546)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat, y_true = 0.5, 0\n",
    "der_sigmoid(y_hat), sigmoid(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6224593312018546"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true*der_sigmoid(y_hat)/sigmoid(y_hat)-(1-y_true)*der_sigmoid(y_hat)/(1-sigmoid(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_log_loss(0.5, 0) == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_log_loss(0.5, 1) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_log_loss(np.array([0.8, 0.8]), np.array([1, 1])) == np.array([1.25, 1.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.82\n"
     ]
    }
   ],
   "source": [
    "print(round(-sum(der_log_loss((x + 1) / 100., x % 2) for x in range(99)), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично теперь мы можем воспользоваться производной сложной функции и получить вычисление градиента формулы по второму варианту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(y_hat, y_true):\n",
    "    return der_log_loss(sigmoid(y_hat), y_true) * der_sigmoid(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем написать код градиентного бустинга для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Допишите класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator # чтобы поддержать интерфейс sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor # для обучения на каждой итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGB(BaseEstimator):\n",
    "    def __init__(self, tree_params_dict, iters=100, tau=1e-1):\n",
    "        \"\"\"\n",
    "        tree_params_dict - словарь параметров, которые надо использовать при обучении дерева на итерации\n",
    "        iters - количество итераций\n",
    "        tau - коэффициент перед предсказаниями деревьев на каждой итерации\n",
    "        \"\"\"\n",
    "        self.tree_params_dict = tree_params_dict\n",
    "        self.iters = iters\n",
    "        self.tau = tau\n",
    "        \n",
    "    def fit(self, X_data, y_data):\n",
    "        self.estimators = []\n",
    "        curr_pred = 0\n",
    "        for iter_num in range(self.iters):\n",
    "            # Нужно найти градиент функции потерь по предсказниям в точке curr_pred\n",
    "            if iter_num == 0:\n",
    "                grad = y_data\n",
    "            else:\n",
    "                grad = calc_gradient(curr_pred, y_data) # TODO\n",
    "            # Мы максимизируем, поэтому надо обучить DecisionTreeRegressor с параметрами \n",
    "            # tree_params_dict по X_data предсказывать grad\n",
    "            algo = DecisionTreeRegressor(**self.tree_params_dict).fit(X_data, grad)\n",
    "            self.estimators.append(algo)\n",
    "            # все предсказания домножаются на tau и обновляется переменная curr_pred\n",
    "            curr_pred += self.tau * algo.predict(X_data)\n",
    "        return curr_pred\n",
    "    \n",
    "    def predict(self, X_data):\n",
    "        # изначально все предскзания нули\n",
    "        res = np.zeros(X_data.shape[0])\n",
    "        for estimator in self.estimators:            \n",
    "            res += estimator.predict(X_data)*self.tau \n",
    "            \n",
    "        return (res > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества полученного класса (в самом низу код для формы)\n",
    "\n",
    "Можете поиграться с параметрами, посмотрим, у кого самое лучшее качество получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оценки качества\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# для генерации датасетов\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# для сравнения\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = make_classification(n_samples=1000, n_features=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SimpleGB(\n",
    "    tree_params_dict={\n",
    "        'max_depth':4\n",
    "    },\n",
    "    iters=100,\n",
    "    tau = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9179999999999999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(algo, X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620287007175179"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(DecisionTreeClassifier(), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9020041751043776"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(XGBClassifier(max_depth=4, eta=.1), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560087002175054"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(LogisticRegression(), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(round(np.mean(cross_val_score(SimpleGB(\n",
    "    tree_params_dict={\n",
    "        'max_depth': 4\n",
    "    },\n",
    "    iters=1000,\n",
    "    tau = 0.01\n",
    "), X_data, y_data, cv=StratifiedKFold(4, random_state=42), scoring='accuracy')), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(cross_val_score(XGBClassifier(max_depth=4, eta=.01), X_data, y_data, cv=StratifiedKFold(4, random_state=42), scoring='accuracy')), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
